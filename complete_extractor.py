"""
ğŸ—ï¸ Ø§Ø¨Ø²Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³Ø§ÛŒØª Ø¨Ø§ Python
Ù†Ø³Ø®Ù‡ Ù…Ø³ØªÙ‚Ù„ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Node.js

Ø§Ø³ØªÙØ§Ø¯Ù‡:
python complete_extractor.py <URL> [OUTPUT_PATH]

Ù…Ø«Ø§Ù„:
python complete_extractor.py https://example.com ./extracted_site
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import re
from urllib.parse import urljoin, urlparse
from pathlib import Path
import time
from datetime import datetime
import shutil
import zipfile

class CompleteSiteExtractor:
    def __init__(self, options=None):
        self.options = options or {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        self.download_images = self.options.get('download_images', True)
        self.download_css = self.options.get('download_css', True) 
        self.download_js = self.options.get('download_js', True)
        self.clean_html = self.options.get('clean_html', True)
        self.timeout = self.options.get('timeout', 30)
        
    def extract_complete_site(self, url, output_path):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³Ø§ÛŒØª"""
        print(f"ğŸ” Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³Ø§ÛŒØª Ø§Ø²: {url}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±Ø¹ÛŒ
        assets_path = output_path / 'assets'
        images_path = assets_path / 'images'
        css_path = assets_path / 'css'
        js_path = assets_path / 'js'
        
        for path in [assets_path, images_path, css_path, js_path]:
            path.mkdir(parents=True, exist_ok=True)
        
        try:
            # Ø¯Ø±ÛŒØ§ÙØª HTML Ø§ØµÙ„ÛŒ
            print("ğŸ“„ Ø¯Ø±ÛŒØ§ÙØª HTML...")
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ØªØ¬Ø²ÛŒÙ‡ URL Ù¾Ø§ÛŒÙ‡
            base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ CSS
            css_files = []
            if self.download_css:
                css_files = self._extract_and_download_css(soup, base_url, css_path)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ JavaScript
            js_files = []
            if self.download_js:
                js_files = self._extract_and_download_js(soup, base_url, js_path)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
            image_files = []
            if self.download_images:
                image_files = self._extract_and_download_images(soup, base_url, images_path)
            
            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ùˆ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ HTML
            if self.clean_html:
                html_content = self._clean_html(html_content)
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ Ø¯Ø± HTML
            html_content = self._update_html_paths(html_content, css_files, js_files, image_files)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§
            metadata = self._extract_metadata(soup, url)
            
            # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            self._save_files(output_path, html_content, metadata, css_files, js_files, image_files)
            
            print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
            print(f"ğŸ“Š Ø¢Ù…Ø§Ø±:")
            print(f"   - CSS: {len(css_files)} ÙØ§ÛŒÙ„")
            print(f"   - JavaScript: {len(js_files)} ÙØ§ÛŒÙ„") 
            print(f"   - ØªØµØ§ÙˆÛŒØ±: {len(image_files)} ÙØ§ÛŒÙ„")
            print(f"ğŸ“ Ø®Ø±ÙˆØ¬ÛŒ: {output_path}")
            
            return {
                'success': True,
                'output_path': str(output_path),
                'stats': {
                    'css_files': len(css_files),
                    'js_files': len(js_files), 
                    'images': len(image_files)
                }
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§: {e}")
            return {'success': False, 'error': str(e)}
    
    def _extract_and_download_css(self, soup, base_url, css_path):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSS"""
        print("ğŸ¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSS...")
        css_files = []
        
        # CSS ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                css_url = urljoin(base_url, href)
                filename = self._get_filename_from_url(css_url, 'css')
                
                if self._download_file(css_url, css_path / filename):
                    css_files.append(filename)
                    link['href'] = f'./assets/css/{filename}'
        
        # CSS inline
        inline_css = []
        for style in soup.find_all('style'):
            if style.string:
                inline_css.append(style.string)
        
        if inline_css:
            inline_filename = 'inline_styles.css'
            with open(css_path / inline_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(inline_css))
            css_files.append(inline_filename)
        
        return css_files
    
    def _extract_and_download_js(self, soup, base_url, js_path):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JavaScript"""
        print("âš¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ JavaScript...")
        js_files = []
        
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src and not src.startswith('data:'):
                js_url = urljoin(base_url, src)
                filename = self._get_filename_from_url(js_url, 'js')
                
                if self._download_file(js_url, js_path / filename):
                    js_files.append(filename)
                    script['src'] = f'./assets/js/{filename}'
        
        return js_files
    
    def _extract_and_download_images(self, soup, base_url, images_path):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±"""
        print("ğŸ–¼ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ±...")
        image_files = []
        
        # ØªØµØ§ÙˆÛŒØ± img
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src and not src.startswith('data:'):
                img_url = urljoin(base_url, src)
                filename = self._get_filename_from_url(img_url, 'jpg')
                
                if self._download_file(img_url, images_path / filename):
                    image_files.append(filename)
                    img['src'] = f'./assets/images/{filename}'
        
        # ØªØµØ§ÙˆÛŒØ± background Ø¯Ø± CSS
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ CSS Ø¯Ø§Ø±Ø¯
        
        return image_files
    
    def _download_file(self, url, file_path):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            print(f"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯: {file_path.name}")
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {url}: {e}")
            return False
    
    def _get_filename_from_url(self, url, default_ext):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø² URL"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        if not filename or '.' not in filename:
            filename = f"file_{hash(url) % 10000}.{default_ext}"
        
        # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù†Ø§Ù… ÙØ§ÛŒÙ„
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename
    
    def _clean_html(self, html):
        """ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† HTML"""
        # Ø­Ø°Ù Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§
        html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)
        
        # Ø­Ø°Ù Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ
        html = re.sub(r'<script[^>]*google-analytics[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<script[^>]*gtag[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        
        # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        html = re.sub(r'\s+', ' ', html)
        
        return html.strip()
    
    def _update_html_paths(self, html, css_files, js_files, image_files):
        """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø³ÛŒØ±Ù‡Ø§ Ø¯Ø± HTML"""
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù†Ø³Ø¨ÛŒ Ø±Ø§ Ø¨Ù‡ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ù†ÛŒØ§Ø² Ø¨Ù‡ regex Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ÛŒ Ø¯Ø§Ø±Ø¯
        return html
    
    def _extract_metadata(self, soup, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§"""
        metadata = {
            'url': url,
            'extracted_at': datetime.now().isoformat(),
            'title': '',
            'description': '',
            'keywords': '',
            'og': {}
        }
        
        # Ø¹Ù†ÙˆØ§Ù†
        title_tag = soup.find('title')
        if title_tag:
            metadata['title'] = title_tag.get_text().strip()
        
        # ØªÙˆØ¶ÛŒØ­Ø§Øª
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag:
            metadata['description'] = desc_tag.get('content', '')
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag:
            metadata['keywords'] = keywords_tag.get('content', '')
        
        # Open Graph
        og_tags = soup.find_all('meta', property=re.compile(r'^og:'))
        for tag in og_tags:
            prop = tag.get('property', '').replace('og:', '')
            content = tag.get('content', '')
            if prop and content:
                metadata['og'][prop] = content
        
        return metadata
    
    def _save_files(self, output_path, html_content, metadata, css_files, js_files, image_files):
        """Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
        
        # Ø°Ø®ÛŒØ±Ù‡ HTML
        with open(output_path / 'index.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
        with open(output_path / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # Ø§ÛŒØ¬Ø§Ø¯ README
        readme_content = f"""# Ø³Ø§ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡

## Ø§Ø·Ù„Ø§Ø¹Ø§Øª
- **URL Ù…Ù†Ø¨Ø¹:** {metadata['url']}
- **Ø¹Ù†ÙˆØ§Ù†:** {metadata['title']}
- **ØªØ§Ø±ÛŒØ® Ø§Ø³ØªØ®Ø±Ø§Ø¬:** {metadata['extracted_at']}

## ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
- `index.html` - ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ
- `metadata.json` - Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§ÛŒØª
- `assets/` - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù†Ø¨ÛŒ
  - `css/` - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ÛŒÙ„ ({len(css_files)} ÙØ§ÛŒÙ„)
  - `js/` - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ({len(js_files)} ÙØ§ÛŒÙ„)
  - `images/` - ØªØµØ§ÙˆÛŒØ± ({len(image_files)} ÙØ§ÛŒÙ„)

## Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
ÙØ§ÛŒÙ„ `index.html` Ø±Ø§ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯.
"""
        
        with open(output_path / 'README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def create_zip(self, folder_path, zip_name=None):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ZIP"""
        folder_path = Path(folder_path)
        
        if not zip_name:
            zip_name = f"{folder_path.name}.zip"
        
        zip_path = folder_path.parent / zip_name
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in folder_path.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(folder_path)
                    zipf.write(file_path, arcname)
        
        print(f"ğŸ“¦ ÙØ§ÛŒÙ„ ZIP Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {zip_path}")
        return str(zip_path)


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    import sys
    
    if len(sys.argv) < 2:
        print("""
ğŸ—ï¸ Ø§Ø¨Ø²Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³Ø§ÛŒØª

Ø§Ø³ØªÙØ§Ø¯Ù‡:
    python complete_extractor.py <URL> [OUTPUT_PATH] [OPTIONS]

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§:
    python complete_extractor.py https://example.com
    python complete_extractor.py https://example.com ./my_site
    
Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§:
    --no-images     Ø¹Ø¯Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
    --no-css        Ø¹Ø¯Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯ CSS
    --no-js         Ø¹Ø¯Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯ JavaScript
    --zip           Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ZIP
        """)
        return
    
    url = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else f"./extracted_{int(time.time())}"
    
    # ØªØ¬Ø²ÛŒÙ‡ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§
    options = {
        'download_images': '--no-images' not in sys.argv,
        'download_css': '--no-css' not in sys.argv,
        'download_js': '--no-js' not in sys.argv,
        'clean_html': True,
        'timeout': 30
    }
    
    # Ø§ÛŒØ¬Ø§Ø¯ extractor
    extractor = CompleteSiteExtractor(options)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬
    result = extractor.extract_complete_site(url, output_path)
    
    if result['success']:
        # Ø§ÛŒØ¬Ø§Ø¯ ZIP Ø¯Ø± ØµÙˆØ±Øª Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        if '--zip' in sys.argv:
            extractor.create_zip(result['output_path'])
        
        print(f"\nğŸ‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙ…Ø§Ù… Ø´Ø¯!")
        print(f"ğŸ“ Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ: {result['output_path']}")
    else:
        print(f"\nâŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…ÙˆÙÙ‚: {result['error']}")


if __name__ == "__main__":
    main()
