#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¢ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ù„ØªÙØ±Ù… Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
Ù…Ø®ØµÙˆØµ Ù‚Ø§Ù„Ø¨â€ŒÙ‡Ø§ØŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØªØŒ Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ Ùˆ Ù†Ù‚Ø´Ù‡â€ŒÙ‡Ø§
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from pathlib import Path
from datetime import datetime

class ProfessionalBusinessExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.base_output_dir = Path("extraction_module/extracted_sites/extracted_sites")
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

    def extract_business_site(self, url, name, category):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§ÛŒØª Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ÛŒ"""
        print(f"ğŸ¢ Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬: {name}")
        print(f"ğŸ”— URL: {url}")
        print(f"ğŸ“‚ Ø¯Ø³ØªÙ‡: {category}")
        
        output_path = self.base_output_dir / name
        output_path.mkdir(parents=True, exist_ok=True)
        
        try:
            # Ø¯Ø±ÛŒØ§ÙØª HTML
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            html_content = response.text
            
            # Ø°Ø®ÛŒØ±Ù‡ HTML
            with open(output_path / "index.html", 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù†
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ØªØ­Ù„ÛŒÙ„ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ÛŒ
            business_analysis = self._analyze_business_features(soup, url)
            
            # Ù…ØªØ§Ø¯ÛŒØªØ§ Ú©Ø§Ù…Ù„
            metadata = {
                "basic_info": {
                    "url": url,
                    "name": name,
                    "category": category,
                    "title": soup.title.string if soup.title else "",
                    "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "size": len(html_content)
                },
                "business_features": business_analysis,
                "seo_analysis": self._analyze_seo(soup),
                "technical_analysis": self._analyze_technical(soup),
                "ui_ux_analysis": self._analyze_ui_ux(soup),
                "extraction_status": "success"
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
            with open(output_path / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # ØªÙˆÙ„ÛŒØ¯ README
            self._generate_business_readme(output_path, metadata)
            
            print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙÙ‚: {name}")
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ {name}: {e}")
            error_metadata = {
                "url": url,
                "name": name,
                "category": category,
                "error": str(e),
                "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "extraction_status": "failed"
            }
            
            with open(output_path / "error.json", 'w', encoding='utf-8') as f:
                json.dump(error_metadata, f, ensure_ascii=False, indent=2)
            
            return False

    def _analyze_business_features(self, soup, url):
        """ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ÛŒ"""
        features = {
            "business_type": self._detect_business_type(soup, url),
            "key_features": [],
            "business_services": [],
            "target_audience": self._detect_target_audience(soup),
            "business_size": self._estimate_business_size(soup),
            "industry_vertical": self._detect_industry(soup, url)
        }
        
        # ØªØ´Ø®ÛŒØµ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
        if self._has_ecommerce(soup):
            features["key_features"].append("E-commerce")
            features["business_services"].append("ÙØ±ÙˆØ´ Ø¢Ù†Ù„Ø§ÛŒÙ†")
        
        if self._has_booking(soup):
            features["key_features"].append("Booking System")
            features["business_services"].append("Ø±Ø²Ø±Ùˆ Ø¢Ù†Ù„Ø§ÛŒÙ†")
        
        if self._has_payment(soup):
            features["key_features"].append("Payment Gateway")
            features["business_services"].append("Ø¯Ø±Ú¯Ø§Ù‡ Ù¾Ø±Ø¯Ø§Ø®Øª")
        
        if self._has_map(soup):
            features["key_features"].append("Map Integration")
            features["business_services"].append("Ù†Ù‚Ø´Ù‡ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª")
        
        if self._has_crm(soup):
            features["key_features"].append("CRM Features")
            features["business_services"].append("Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø´ØªØ±ÛŒ")
        
        if self._has_inventory(soup):
            features["key_features"].append("Inventory Management")
            features["business_services"].append("Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ")
        
        if self._has_accounting(soup):
            features["key_features"].append("Accounting System")
            features["business_services"].append("Ø³ÛŒØ³ØªÙ… Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ")
        
        return features

    def _detect_business_type(self, soup, url):
        """ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±"""
        text = soup.get_text().lower()
        url_lower = url.lower()
        
        business_keywords = {
            "restaurant": ["restaurant", "food", "menu", "dining", "cafe"],
            "retail": ["shop", "store", "buy", "product", "cart"],
            "service": ["service", "appointment", "booking", "consultation"],
            "healthcare": ["health", "medical", "doctor", "clinic", "hospital"],
            "beauty": ["beauty", "salon", "spa", "hair", "makeup"],
            "education": ["education", "school", "course", "training", "learn"],
            "finance": ["finance", "bank", "payment", "accounting", "invoice"],
            "real_estate": ["property", "real estate", "house", "apartment"],
            "automotive": ["car", "auto", "vehicle", "repair", "garage"],
            "technology": ["tech", "software", "app", "digital", "it"]
        }
        
        for biz_type, keywords in business_keywords.items():
            if any(keyword in text or keyword in url_lower for keyword in keywords):
                return biz_type
        
        return "general"

    def _detect_target_audience(self, soup):
        """ØªØ´Ø®ÛŒØµ Ù…Ø®Ø§Ø·Ø¨ Ù‡Ø¯Ù"""
        text = soup.get_text().lower()
        
        if any(word in text for word in ["enterprise", "corporation", "business", "company"]):
            return "B2B"
        elif any(word in text for word in ["government", "public", "municipal"]):
            return "B2G"
        else:
            return "B2C"

    def _estimate_business_size(self, soup):
        """ØªØ®Ù…ÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±"""
        text = soup.get_text().lower()
        
        if any(word in text for word in ["enterprise", "corporation", "global", "international"]):
            return "large"
        elif any(word in text for word in ["medium", "regional", "multi-location"]):
            return "medium"
        elif any(word in text for word in ["small", "local", "family", "startup"]):
            return "small"
        else:
            return "micro"

    def _detect_industry(self, soup, url):
        """ØªØ´Ø®ÛŒØµ ØµÙ†Ø¹Øª"""
        text = soup.get_text().lower()
        url_lower = url.lower()
        
        industries = {
            "technology": ["tech", "software", "digital", "ai", "cloud"],
            "healthcare": ["health", "medical", "care", "wellness"],
            "finance": ["finance", "bank", "invest", "money", "payment"],
            "retail": ["retail", "shop", "store", "commerce"],
            "food": ["food", "restaurant", "cafe", "dining"],
            "education": ["education", "school", "university", "training"],
            "real_estate": ["property", "real estate", "construction"],
            "automotive": ["auto", "car", "vehicle", "transport"],
            "beauty": ["beauty", "salon", "cosmetic", "spa"],
            "manufacturing": ["manufacturing", "production", "factory"]
        }
        
        for industry, keywords in industries.items():
            if any(keyword in text or keyword in url_lower for keyword in keywords):
                return industry
        
        return "general"

    def _has_ecommerce(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ†"""
        indicators = ["add to cart", "buy now", "checkout", "shopping cart", "price", "$"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _has_booking(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… Ø±Ø²Ø±Ùˆ"""
        indicators = ["book", "appointment", "reserve", "schedule", "availability"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _has_payment(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ú¯Ø§Ù‡ Ù¾Ø±Ø¯Ø§Ø®Øª"""
        indicators = ["payment", "pay", "checkout", "credit card", "paypal", "stripe"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _has_map(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù‚Ø´Ù‡"""
        return bool(soup.find_all(['iframe', 'div'], attrs={'src': lambda x: x and 'maps' in x}) or
                   soup.find_all(attrs={'class': lambda x: x and 'map' in str(x).lower()}))

    def _has_crm(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ CRM"""
        indicators = ["crm", "customer management", "lead", "contact management"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _has_inventory(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ"""
        indicators = ["inventory", "stock", "warehouse", "product management"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _has_accounting(self, soup):
        """Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ"""
        indicators = ["accounting", "invoice", "billing", "financial", "bookkeeping"]
        text = soup.get_text().lower()
        return any(indicator in text for indicator in indicators)

    def _analyze_seo(self, soup):
        """ØªØ­Ù„ÛŒÙ„ SEO"""
        return {
            "meta_description": self._get_meta_content(soup, "description"),
            "meta_keywords": self._get_meta_content(soup, "keywords"),
            "og_title": self._get_meta_property(soup, "og:title"),
            "og_description": self._get_meta_property(soup, "og:description"),
            "h1_count": len(soup.find_all('h1')),
            "h2_count": len(soup.find_all('h2')),
            "img_alt_missing": len([img for img in soup.find_all('img') if not img.get('alt')])
        }

    def _analyze_technical(self, soup):
        """ØªØ­Ù„ÛŒÙ„ ÙÙ†ÛŒ"""
        return {
            "has_responsive": bool(soup.find('meta', attrs={'name': 'viewport'})),
            "css_files": len(soup.find_all('link', attrs={'rel': 'stylesheet'})),
            "js_files": len(soup.find_all('script', attrs={'src': True})),
            "external_links": len([a for a in soup.find_all('a', href=True) if 'http' in a['href']]),
            "forms": len(soup.find_all('form')),
            "has_ssl": True  # Ø§Ú¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ SSL Ø¯Ø§Ø±Ø¯
        }

    def _analyze_ui_ux(self, soup):
        """ØªØ­Ù„ÛŒÙ„ UI/UX"""
        return {
            "navigation_menus": len(soup.find_all(['nav', 'ul', 'ol'])),
            "buttons": len(soup.find_all(['button', 'input'], attrs={'type': 'submit'})),
            "images": len(soup.find_all('img')),
            "videos": len(soup.find_all(['video', 'iframe'])),
            "contact_forms": len([form for form in soup.find_all('form') 
                                if any(field in str(form).lower() for field in ['email', 'contact', 'message'])])
        }

    def _get_meta_content(self, soup, name):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ meta tag"""
        meta = soup.find('meta', attrs={'name': name})
        return meta.get('content', '') if meta else ''

    def _get_meta_property(self, soup, property_name):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ meta property"""
        meta = soup.find('meta', attrs={'property': property_name})
        return meta.get('content', '') if meta else ''

    def _generate_business_readme(self, output_path, metadata):
        """ØªÙˆÙ„ÛŒØ¯ README Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ø§ÛŒØª"""
        basic = metadata['basic_info']
        business = metadata['business_features']
        
        readme_content = f"""# {basic['name']}

## ğŸ“Š Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ
- **URL:** {basic['url']}
- **Ø¯Ø³ØªÙ‡:** {basic['category']}
- **ØªØ§Ø±ÛŒØ® Ø§Ø³ØªØ®Ø±Ø§Ø¬:** {basic['extracted_at']}
- **Ø§Ù†Ø¯Ø§Ø²Ù‡:** {basic['size']:,} Ø¨Ø§ÛŒØª

## ğŸ¢ ØªØ­Ù„ÛŒÙ„ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ÛŒ
- **Ù†ÙˆØ¹ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±:** {business['business_type']}
- **Ø§Ù†Ø¯Ø§Ø²Ù‡:** {business['business_size']}
- **ØµÙ†Ø¹Øª:** {business['industry_vertical']}
- **Ù…Ø®Ø§Ø·Ø¨:** {business['target_audience']}

## â­ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
{chr(10).join(f"- {feature}" for feature in business['key_features'])}

## ğŸ”§ Ø®Ø¯Ù…Ø§Øª Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ÛŒ
{chr(10).join(f"- {service}" for service in business['business_services'])}

## ğŸ¯ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¨Ø±Ø§ÛŒ Ù‚Ø§Ù„Ø¨
Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ø¨Ø±Ø§ÛŒ {business['business_type']} Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ù‚Ø§Ù„Ø¨â€ŒÙ‡Ø§ÛŒ:
- Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±Ù‡Ø§ÛŒ {business['business_size']}
- ØµÙ†Ø¹Øª {business['industry_vertical']}
- Ù…Ø®Ø§Ø·Ø¨Ø§Ù† {business['target_audience']}

---
*Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Professional Business Extractor*
"""
        
        with open(output_path / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)

def main():
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±"""
    extractor = ProfessionalBusinessExtractor()
    
    # Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù„Ø¨ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
    template_sites = [
        # Ù‚Ø§Ù„Ø¨â€ŒÙ‡Ø§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø± Ú©ÙˆÚ†Ú©
        ("https://themeforest.net", "themeforest_templates", "business_templates"),
        ("https://templatemonster.com", "templatemonster", "business_templates"),
        ("https://colorlib.com", "colorlib_templates", "business_templates"),
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±
        ("https://freshbooks.com", "freshbooks_accounting", "accounting_software"),
        ("https://quickbooks.intuit.com", "quickbooks", "accounting_software"),
        ("https://www.zoho.com", "zoho_business", "business_management"),
        ("https://monday.com", "monday_project", "project_management"),
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ Ùˆ CRM
        ("https://shopify.com", "shopify_ecommerce", "ecommerce_platform"),
        ("https://woocommerce.com", "woocommerce", "ecommerce_platform"),
        ("https://salesforce.com", "salesforce_crm", "crm_system"),
        ("https://hubspot.com", "hubspot_crm", "crm_system"),
        
        # Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‚Ø´Ù‡ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª
        ("https://foursquare.com", "foursquare_business", "location_services"),
        ("https://business.google.com", "google_business", "business_listing"),
        ("https://www.yelp.com/biz", "yelp_business", "business_directory"),
        
        # Ø±Ø³ØªÙˆØ±Ø§Ù† Ùˆ ØºØ°Ø§
        ("https://www.opentable.com", "opentable_restaurant", "restaurant_booking"),
        ("https://www.ubereats.com", "ubereats", "food_delivery"),
        ("https://www.doordash.com", "doordash", "food_delivery"),
        
        # Ø²ÛŒØ¨Ø§ÛŒÛŒ Ùˆ Ø³Ù„Ø§Ù…Øª
        ("https://www.vagaro.com", "vagaro_salon", "salon_booking"),
        ("https://www.booksy.com", "booksy_beauty", "beauty_booking"),
        ("https://www.fresha.com", "fresha_wellness", "wellness_booking"),
        
        # Ø®Ø±Ø¯Ù‡â€ŒÙØ±ÙˆØ´ÛŒ
        ("https://square.com", "square_retail", "retail_pos"),
        ("https://www.lightspeedhq.com", "lightspeed_retail", "retail_management"),
        ("https://www.shopkeep.com", "shopkeep", "small_business_pos"),
        
        # Ù¾Ø²Ø´Ú©ÛŒ
        ("https://www.practicefusion.com", "practice_fusion", "medical_practice"),
        ("https://www.athenahealth.com", "athena_health", "healthcare_management"),
        ("https://www.doximity.com", "doximity_medical", "medical_network"),
        
        # Ø¢Ù…ÙˆØ²Ø´
        ("https://www.teachable.com", "teachable_education", "online_education"),
        ("https://www.udemy.com", "udemy_courses", "online_learning"),
        ("https://www.coursera.org", "coursera", "online_education"),
    ]
    
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±...")
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§: {len(template_sites)}")
    
    success_count = 0
    total_sites = len(template_sites)
    
    for i, (url, name, category) in enumerate(template_sites, 1):
        print(f"\n[{i}/{total_sites}] ğŸ¯ {name}")
        if extractor.extract_business_site(url, name, category):
            success_count += 1
        
        if i < total_sites:
            print("â³ Ø§Ù†ØªØ¸Ø§Ø± 3 Ø«Ø§Ù†ÛŒÙ‡...")
            time.sleep(3)
    
    # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
    print(f"\nğŸ‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print(f"âœ… Ù…ÙˆÙÙ‚: {success_count}/{total_sites}")
    print(f"ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø±: extraction_module/extracted_sites/extracted_sites/")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
    report = {
        "extraction_type": "Professional Business Sites",
        "total_sites": total_sites,
        "successful": success_count,
        "failed": total_sites - success_count,
        "success_rate": f"{(success_count/total_sites)*100:.1f}%",
        "extraction_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "categories": {
            "business_templates": 3,
            "accounting_software": 3,
            "business_management": 2,
            "ecommerce_platform": 3,
            "crm_system": 2,
            "location_services": 3,
            "restaurant_booking": 3,
            "beauty_booking": 3,
            "retail_management": 3,
            "medical_practice": 3,
            "online_education": 3
        }
    }
    
    with open("extraction_module/professional_business_extraction_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: professional_business_extraction_report.json")

if __name__ == "__main__":
    main()