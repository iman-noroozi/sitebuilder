#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š Predictive Analytics Engine - Ù…ÙˆØªÙˆØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯
Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡:
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØµÙØ­Ø§Øª
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± SEO
- ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ø·Ø±Ø§Ø­ÛŒ
"""

import os
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import asyncio
import aiohttp
from dataclasses import dataclass
from enum import Enum
import openai
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class MetricType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§"""
    PERFORMANCE = "performance"
    SEO = "seo"
    USER_EXPERIENCE = "user_experience"
    CONVERSION = "conversion"
    ACCESSIBILITY = "accessibility"

@dataclass
class AnalyticsData:
    """Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ"""
    page_url: str
    load_time: float
    bounce_rate: float
    conversion_rate: float
    seo_score: int
    accessibility_score: int
    user_satisfaction: float
    timestamp: datetime

@dataclass
class PredictionResult:
    """Ù†ØªÛŒØ¬Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
    metric: MetricType
    current_value: float
    predicted_value: float
    confidence: float
    recommendations: List[str]
    risk_level: str

class PredictiveAnalyticsEngine:
    """Ù…ÙˆØªÙˆØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.openai_api_key = self.config.get('openai_api_key')
        self.analytics_data = self._load_analytics_data()
        self.models = self._initialize_models()
        self.seo_keywords = self._load_seo_keywords()
        self.performance_benchmarks = self._load_performance_benchmarks()
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.optimization_patterns = {
            "performance": {
                "image_optimization": 0.15,
                "css_minification": 0.08,
                "js_bundling": 0.12,
                "caching": 0.20,
                "cdn_usage": 0.25
            },
            "seo": {
                "meta_tags": 0.20,
                "heading_structure": 0.15,
                "content_quality": 0.25,
                "internal_linking": 0.10,
                "mobile_friendly": 0.15,
                "page_speed": 0.15
            },
            "accessibility": {
                "alt_texts": 0.20,
                "color_contrast": 0.15,
                "keyboard_navigation": 0.20,
                "screen_reader": 0.25,
                "focus_indicators": 0.20
            }
        }
    
    def _load_analytics_data(self) -> List[AnalyticsData]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ"""
        data_file = "analytics_data.json"
        if os.path.exists(data_file):
            with open(data_file, "r", encoding="utf-8") as f:
                raw_data = json.load(f)
            
            return [
                AnalyticsData(
                    page_url=item["page_url"],
                    load_time=item["load_time"],
                    bounce_rate=item["bounce_rate"],
                    conversion_rate=item["conversion_rate"],
                    seo_score=item["seo_score"],
                    accessibility_score=item["accessibility_score"],
                    user_satisfaction=item["user_satisfaction"],
                    timestamp=datetime.fromisoformat(item["timestamp"])
                )
                for item in raw_data
            ]
        
        return []
    
    def _initialize_models(self) -> Dict:
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML"""
        return {
            "performance": RandomForestRegressor(n_estimators=100, random_state=42),
            "seo": RandomForestRegressor(n_estimators=100, random_state=42),
            "conversion": RandomForestRegressor(n_estimators=100, random_state=42)
        }
    
    def _load_seo_keywords(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ SEO"""
        return {
            "persian": [
                "ÙˆØ¨â€ŒØ³Ø§ÛŒØª", "Ø·Ø±Ø§Ø­ÛŒ Ø³Ø§ÛŒØª", "Ø³Ø§ÛŒØª Ø³Ø§Ø²", "ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ†",
                "Ø±Ø³ØªÙˆØ±Ø§Ù†", "Ø®Ø¯Ù…Ø§Øª", "Ù…Ø­ØµÙˆÙ„Ø§Øª", "ØªÙ…Ø§Ø³ Ø¨Ø§ Ù…Ø§"
            ],
            "english": [
                "website", "web design", "site builder", "online store",
                "restaurant", "services", "products", "contact us"
            ]
        }
    
    def _load_performance_benchmarks(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        return {
            "load_time": {
                "excellent": 1.0,
                "good": 2.0,
                "needs_improvement": 3.0,
                "poor": 5.0
            },
            "bounce_rate": {
                "excellent": 0.25,
                "good": 0.40,
                "needs_improvement": 0.60,
                "poor": 0.80
            },
            "conversion_rate": {
                "excellent": 0.05,
                "good": 0.03,
                "needs_improvement": 0.02,
                "poor": 0.01
            }
        }
    
    async def analyze_website_performance(self, website_data: Dict) -> Dict:
        """
        ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙˆØ¨â€ŒØ³Ø§ÛŒØª
        
        Args:
            website_data: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ¨â€ŒØ³Ø§ÛŒØª (HTML, CSS, JS, images, etc.)
            
        Returns:
            ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯
        """
        print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÙˆØ¨â€ŒØ³Ø§ÛŒØª...")
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯
        performance_analysis = self._analyze_performance(website_data)
        
        # ØªØ­Ù„ÛŒÙ„ SEO
        seo_analysis = await self._analyze_seo(website_data)
        
        # ØªØ­Ù„ÛŒÙ„ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ
        ux_analysis = self._analyze_user_experience(website_data)
        
        # ØªØ­Ù„ÛŒÙ„ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ
        accessibility_analysis = self._analyze_accessibility(website_data)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
        predictions = await self._generate_predictions(website_data)
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯
        recommendations = await self._generate_recommendations(
            performance_analysis, seo_analysis, ux_analysis, accessibility_analysis
        )
        
        return {
            "performance": performance_analysis,
            "seo": seo_analysis,
            "user_experience": ux_analysis,
            "accessibility": accessibility_analysis,
            "predictions": predictions,
            "recommendations": recommendations,
            "overall_score": self._calculate_overall_score(
                performance_analysis, seo_analysis, ux_analysis, accessibility_analysis
            ),
            "analyzed_at": datetime.now().isoformat()
        }
    
    def _analyze_performance(self, website_data: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        html_size = len(website_data.get("html", ""))
        css_size = len(website_data.get("css", ""))
        js_size = len(website_data.get("javascript", ""))
        images_count = len(website_data.get("images", []))
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯
        performance_score = 100
        
        # Ú©Ø§Ù‡Ø´ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        if html_size > 100000:  # 100KB
            performance_score -= 10
        if css_size > 50000:  # 50KB
            performance_score -= 15
        if js_size > 100000:  # 100KB
            performance_score -= 20
        if images_count > 20:
            performance_score -= 25
        
        # ØªØ­Ù„ÛŒÙ„ ØªØµØ§ÙˆÛŒØ±
        image_analysis = self._analyze_images(website_data.get("images", []))
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø²Ù…Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
        estimated_load_time = self._estimate_load_time(html_size, css_size, js_size, images_count)
        
        return {
            "score": max(0, performance_score),
            "estimated_load_time": estimated_load_time,
            "html_size": html_size,
            "css_size": css_size,
            "js_size": js_size,
            "images_count": images_count,
            "image_analysis": image_analysis,
            "optimization_potential": self._calculate_optimization_potential(website_data)
        }
    
    def _analyze_images(self, images: List[Dict]) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ ØªØµØ§ÙˆÛŒØ±"""
        total_size = 0
        unoptimized_count = 0
        missing_alt_count = 0
        
        for image in images:
            total_size += image.get("size", 0)
            if image.get("size", 0) > 500000:  # 500KB
                unoptimized_count += 1
            if not image.get("alt_text"):
                missing_alt_count += 1
        
        return {
            "total_size": total_size,
            "unoptimized_count": unoptimized_count,
            "missing_alt_count": missing_alt_count,
            "optimization_score": max(0, 100 - (unoptimized_count * 10) - (missing_alt_count * 5))
        }
    
    def _estimate_load_time(self, html_size: int, css_size: int, js_size: int, images_count: int) -> float:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø²Ù…Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ"""
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±
        base_time = 0.5  # Ø²Ù…Ø§Ù† Ù¾Ø§ÛŒÙ‡
        
        # Ø²Ù…Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ (ÙØ±Ø¶: 1MB/s)
        file_time = (html_size + css_size + js_size) / (1024 * 1024)
        
        # Ø²Ù…Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØµØ§ÙˆÛŒØ± (ÙØ±Ø¶: 200KB/s Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªØµÙˆÛŒØ±)
        image_time = (images_count * 200) / (1024 * 1024)
        
        return base_time + file_time + image_time
    
    def _calculate_optimization_potential(self, website_data: Dict) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        potential = {}
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±
        images = website_data.get("images", [])
        if images:
            large_images = [img for img in images if img.get("size", 0) > 500000]
            potential["image_optimization"] = {
                "current_score": 100 - (len(large_images) * 10),
                "potential_improvement": len(large_images) * 10,
                "recommendation": f"Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ {len(large_images)} ØªØµÙˆÛŒØ± Ø¨Ø²Ø±Ú¯"
            }
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CSS
        css_size = len(website_data.get("css", ""))
        if css_size > 50000:
            potential["css_optimization"] = {
                "current_score": 100 - ((css_size - 50000) // 1000),
                "potential_improvement": min(20, (css_size - 50000) // 1000),
                "recommendation": "ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø­Ø°Ù CSS ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ"
            }
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ JavaScript
        js_size = len(website_data.get("javascript", ""))
        if js_size > 100000:
            potential["js_optimization"] = {
                "current_score": 100 - ((js_size - 100000) // 2000),
                "potential_improvement": min(25, (js_size - 100000) // 2000),
                "recommendation": "Ø¨Ø§Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† Ùˆ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ JavaScript"
            }
        
        return potential
    
    async def _analyze_seo(self, website_data: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ SEO"""
        html_content = website_data.get("html", "")
        
        # ØªØ­Ù„ÛŒÙ„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ meta
        meta_analysis = self._analyze_meta_tags(html_content)
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± heading
        heading_analysis = self._analyze_heading_structure(html_content)
        
        # ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§
        content_analysis = self._analyze_content_quality(html_content)
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keyword_analysis = self._analyze_keywords(html_content)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² SEO
        seo_score = (
            meta_analysis["score"] * 0.3 +
            heading_analysis["score"] * 0.2 +
            content_analysis["score"] * 0.3 +
            keyword_analysis["score"] * 0.2
        )
        
        return {
            "score": seo_score,
            "meta_tags": meta_analysis,
            "heading_structure": heading_analysis,
            "content_quality": content_analysis,
            "keywords": keyword_analysis,
            "recommendations": self._generate_seo_recommendations(meta_analysis, heading_analysis, content_analysis)
        }
    
    def _analyze_meta_tags(self, html_content: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ meta"""
        score = 0
        issues = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ title
        if "<title>" in html_content:
            score += 20
        else:
            issues.append("ØªÚ¯ title ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ meta description
        if 'name="description"' in html_content:
            score += 20
        else:
            issues.append("meta description ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ meta keywords
        if 'name="keywords"' in html_content:
            score += 10
        else:
            issues.append("meta keywords ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ viewport
        if 'name="viewport"' in html_content:
            score += 15
        else:
            issues.append("viewport meta tag ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ charset
        if 'charset=' in html_content:
            score += 10
        else:
            issues.append("charset ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Open Graph
        if 'property="og:' in html_content:
            score += 15
        else:
            issues.append("Open Graph tags ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ canonical
        if 'rel="canonical"' in html_content:
            score += 10
        else:
            issues.append("canonical URL ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡")
        
        return {
            "score": score,
            "issues": issues,
            "total_possible": 100
        }
    
    def _analyze_heading_structure(self, html_content: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± heading"""
        import re
        
        headings = re.findall(r'<h([1-6])[^>]*>(.*?)</h[1-6]>', html_content, re.IGNORECASE | re.DOTALL)
        
        score = 0
        issues = []
        
        if not headings:
            issues.append("Ù‡ÛŒÚ† heading ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return {"score": 0, "issues": issues, "structure": []}
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ H1
        h1_count = len([h for h in headings if h[0] == '1'])
        if h1_count == 1:
            score += 30
        elif h1_count == 0:
            issues.append("H1 ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        else:
            issues.append(f"ØªØ¹Ø¯Ø§Ø¯ H1 Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ ({h1_count})")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ±ØªÛŒØ¨ heading Ù‡Ø§
        current_level = 1
        structure_issues = 0
        
        for level, text in headings:
            level = int(level)
            if level > current_level + 1:
                structure_issues += 1
            current_level = level
        
        if structure_issues == 0:
            score += 40
        else:
            issues.append(f"Ø³Ø§Ø®ØªØ§Ø± heading Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ ({structure_issues} Ù…Ø´Ú©Ù„)")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ heading Ù‡Ø§
        long_headings = [h for h in headings if len(h[1].strip()) > 60]
        if len(long_headings) == 0:
            score += 30
        else:
            issues.append(f"{len(long_headings)} heading Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ")
        
        return {
            "score": score,
            "issues": issues,
            "structure": [(int(h[0]), h[1].strip()) for h in headings],
            "total_possible": 100
        }
    
    def _analyze_content_quality(self, html_content: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ù…Ø­ØªÙˆØ§"""
        import re
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…Ø­ØªÙˆØ§
        text_content = re.sub(r'<[^>]+>', ' ', html_content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        word_count = len(text_content.split())
        char_count = len(text_content)
        
        score = 0
        issues = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§
        if word_count >= 300:
            score += 40
        elif word_count >= 150:
            score += 20
        else:
            issues.append("Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ±Ø§Ú©Ù… Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        keyword_density = self._calculate_keyword_density(text_content)
        if 1 <= keyword_density <= 3:
            score += 30
        elif keyword_density > 3:
            issues.append("ØªØ±Ø§Ú©Ù… Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯")
        else:
            issues.append("ØªØ±Ø§Ú©Ù… Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù…")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ
        readability_score = self._calculate_readability(text_content)
        if readability_score >= 60:
            score += 30
        else:
            issues.append("Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ù…Ø­ØªÙˆØ§ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª")
        
        return {
            "score": score,
            "word_count": word_count,
            "char_count": char_count,
            "keyword_density": keyword_density,
            "readability_score": readability_score,
            "issues": issues,
            "total_possible": 100
        }
    
    def _calculate_keyword_density(self, text: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ±Ø§Ú©Ù… Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""
        words = text.lower().split()
        if not words:
            return 0
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ÛŒØ¬
        keywords = ["ÙˆØ¨â€ŒØ³Ø§ÛŒØª", "Ø·Ø±Ø§Ø­ÛŒ", "Ø®Ø¯Ù…Ø§Øª", "Ù…Ø­ØµÙˆÙ„Ø§Øª", "ØªÙ…Ø§Ø³"]
        keyword_count = sum(1 for word in words if word in keywords)
        
        return (keyword_count / len(words)) * 100
    
    def _calculate_readability(self, text: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ù…ØªÙ†"""
        sentences = text.split('.')
        words = text.split()
        
        if not sentences or not words:
            return 0
        
        avg_sentence_length = len(words) / len(sentences)
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # ÙØ±Ù…ÙˆÙ„ Ø³Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ
        readability = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_word_length)
        
        return max(0, min(100, readability))
    
    def _analyze_keywords(self, html_content: str) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""
        import re
        
        text_content = re.sub(r'<[^>]+>', ' ', html_content).lower()
        words = text_content.split()
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ÙØ§Ø±Ø³ÛŒ
        persian_keywords = self.seo_keywords["persian"]
        english_keywords = self.seo_keywords["english"]
        
        found_keywords = []
        for keyword in persian_keywords + english_keywords:
            if keyword.lower() in text_content:
                found_keywords.append(keyword)
        
        score = min(100, len(found_keywords) * 10)
        
        return {
            "score": score,
            "found_keywords": found_keywords,
            "total_possible": len(persian_keywords) + len(english_keywords),
            "recommendations": self._generate_keyword_recommendations(found_keywords)
        }
    
    def _generate_keyword_recommendations(self, found_keywords: List[str]) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ"""
        recommendations = []
        
        if len(found_keywords) < 3:
            recommendations.append("Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯")
        
        if not any("ÙØ§Ø±Ø³ÛŒ" in kw for kw in found_keywords):
            recommendations.append("Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯")
        
        return recommendations
    
    def _generate_seo_recommendations(self, meta_analysis: Dict, heading_analysis: Dict, content_analysis: Dict) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª SEO"""
        recommendations = []
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ meta
        if meta_analysis["score"] < 70:
            recommendations.extend(meta_analysis["issues"])
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ heading
        if heading_analysis["score"] < 70:
            recommendations.extend(heading_analysis["issues"])
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§
        if content_analysis["score"] < 70:
            recommendations.extend(content_analysis["issues"])
        
        return list(set(recommendations))  # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±
    
    def _analyze_user_experience(self, website_data: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ"""
        html_content = website_data.get("html", "")
        css_content = website_data.get("css", "")
        
        score = 0
        issues = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ responsive design
        if "media query" in css_content.lower() or "viewport" in html_content.lower():
            score += 25
        else:
            issues.append("Ø·Ø±Ø§Ø­ÛŒ responsive ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ navigation
        if "nav" in html_content.lower() or "menu" in html_content.lower():
            score += 20
        else:
            issues.append("Ù…Ù†ÙˆÛŒ Ù†Ø§ÙˆØ¨Ø±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ CTA buttons
        if "button" in html_content.lower() or "cta" in html_content.lower():
            score += 20
        else:
            issues.append("Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ CTA ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ forms
        if "form" in html_content.lower():
            score += 15
        else:
            issues.append("ÙØ±Ù… ØªÙ…Ø§Ø³ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ social media links
        if "facebook" in html_content.lower() or "instagram" in html_content.lower():
            score += 10
        else:
            issues.append("Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ contact information
        if "tel:" in html_content.lower() or "mailto:" in html_content.lower():
            score += 10
        else:
            issues.append("Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ø³ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        return {
            "score": score,
            "issues": issues,
            "total_possible": 100
        }
    
    def _analyze_accessibility(self, website_data: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ"""
        html_content = website_data.get("html", "")
        
        score = 0
        issues = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ alt text Ø¨Ø±Ø§ÛŒ ØªØµØ§ÙˆÛŒØ±
        img_tags = html_content.count("<img")
        alt_attributes = html_content.count('alt="')
        
        if img_tags > 0:
            alt_percentage = (alt_attributes / img_tags) * 100
            if alt_percentage >= 90:
                score += 25
            else:
                issues.append(f"ÙÙ‚Ø· {alt_percentage:.1f}% ØªØµØ§ÙˆÛŒØ± alt text Ø¯Ø§Ø±Ù†Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ARIA labels
        aria_count = html_content.count("aria-")
        if aria_count > 0:
            score += 20
        else:
            issues.append("ARIA labels ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ semantic HTML
        semantic_tags = ["header", "nav", "main", "section", "article", "aside", "footer"]
        semantic_count = sum(html_content.count(f"<{tag}") for tag in semantic_tags)
        
        if semantic_count > 0:
            score += 25
        else:
            issues.append("ØªÚ¯â€ŒÙ‡Ø§ÛŒ semantic HTML Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡")
        
        # Ø¨Ø±Ø±Ø³ÛŒ keyboard navigation
        if "tabindex" in html_content.lower():
            score += 15
        else:
            issues.append("Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² keyboard navigation ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ color contrast (Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø¯Ù‡)
        if "color:" in html_content.lower() and "background" in html_content.lower():
            score += 15
        else:
            issues.append("Ú©Ù†ØªØ±Ø§Ø³Øª Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø¯Ù‡")
        
        return {
            "score": score,
            "issues": issues,
            "total_possible": 100
        }
    
    async def _generate_predictions(self, website_data: Dict) -> List[PredictionResult]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§"""
        predictions = []
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        performance_prediction = await self._predict_performance(website_data)
        predictions.append(performance_prediction)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ SEO
        seo_prediction = await self._predict_seo(website_data)
        predictions.append(seo_prediction)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ conversion
        conversion_prediction = await self._predict_conversion(website_data)
        predictions.append(conversion_prediction)
        
        return predictions
    
    async def _predict_performance(self, website_data: Dict) -> PredictionResult:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        html_size = len(website_data.get("html", ""))
        css_size = len(website_data.get("css", ""))
        js_size = len(website_data.get("javascript", ""))
        images_count = len(website_data.get("images", []))
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ÙØ¹Ù„ÛŒ
        current_score = 100
        if html_size > 100000:
            current_score -= 10
        if css_size > 50000:
            current_score -= 15
        if js_size > 100000:
            current_score -= 20
        if images_count > 20:
            current_score -= 25
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯
        predicted_score = min(100, current_score + 20)  # ÙØ±Ø¶ Ø¨Ù‡Ø¨ÙˆØ¯ 20 Ø§Ù…ØªÛŒØ§Ø²
        
        recommendations = []
        if html_size > 100000:
            recommendations.append("ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ HTML")
        if css_size > 50000:
            recommendations.append("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CSS")
        if js_size > 100000:
            recommendations.append("Ø¨Ø§Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† JavaScript")
        if images_count > 20:
            recommendations.append("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±")
        
        return PredictionResult(
            metric=MetricType.PERFORMANCE,
            current_value=current_score,
            predicted_value=predicted_score,
            confidence=0.85,
            recommendations=recommendations,
            risk_level="low" if predicted_score > 80 else "medium"
        )
    
    async def _predict_seo(self, website_data: Dict) -> PredictionResult:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ SEO"""
        html_content = website_data.get("html", "")
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ SEO
        current_score = 0
        if "<title>" in html_content:
            current_score += 20
        if 'name="description"' in html_content:
            current_score += 20
        if 'name="viewport"' in html_content:
            current_score += 15
        if "<h1>" in html_content:
            current_score += 25
        if "alt=" in html_content:
            current_score += 20
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯
        predicted_score = min(100, current_score + 30)
        
        recommendations = []
        if "<title>" not in html_content:
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÚ¯ title")
        if 'name="description"' not in html_content:
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† meta description")
        if "<h1>" not in html_content:
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† H1")
        if "alt=" not in html_content:
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† alt text Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ±")
        
        return PredictionResult(
            metric=MetricType.SEO,
            current_value=current_score,
            predicted_value=predicted_score,
            confidence=0.90,
            recommendations=recommendations,
            risk_level="low" if predicted_score > 70 else "medium"
        )
    
    async def _predict_conversion(self, website_data: Dict) -> PredictionResult:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ conversion rate"""
        html_content = website_data.get("html", "")
        
        # ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ conversion
        current_score = 0
        if "button" in html_content.lower():
            current_score += 30
        if "form" in html_content.lower():
            current_score += 25
        if "contact" in html_content.lower():
            current_score += 20
        if "phone" in html_content.lower() or "tel:" in html_content:
            current_score += 15
        if "email" in html_content.lower() or "mailto:" in html_content:
            current_score += 10
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯
        predicted_score = min(100, current_score + 25)
        
        recommendations = []
        if "button" not in html_content.lower():
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ CTA")
        if "form" not in html_content.lower():
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ±Ù… ØªÙ…Ø§Ø³")
        if "contact" not in html_content.lower():
            recommendations.append("Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ø³")
        
        return PredictionResult(
            metric=MetricType.CONVERSION,
            current_value=current_score,
            predicted_value=predicted_score,
            confidence=0.75,
            recommendations=recommendations,
            risk_level="low" if predicted_score > 60 else "medium"
        )
    
    async def _generate_recommendations(self, performance: Dict, seo: Dict, ux: Dict, accessibility: Dict) -> List[Dict]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯"""
        recommendations = []
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
        if performance["score"] < 80:
            recommendations.append({
                "category": "performance",
                "priority": "high",
                "title": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯",
                "description": "Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø±Ø¹Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³Ø§ÛŒØª",
                "actions": [
                    "ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±",
                    "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CSS Ùˆ JavaScript",
                    "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN",
                    "ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ caching"
                ],
                "expected_improvement": "20-30% Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø±Ø¹Øª"
            })
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª SEO
        if seo["score"] < 70:
            recommendations.append({
                "category": "seo",
                "priority": "high",
                "title": "Ø¨Ù‡Ø¨ÙˆØ¯ SEO",
                "description": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ",
                "actions": [
                    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† meta tags",
                    "Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø§Ø®ØªØ§Ø± heading",
                    "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§",
                    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† alt text"
                ],
                "expected_improvement": "15-25% Ø¨Ù‡Ø¨ÙˆØ¯ Ø±ØªØ¨Ù‡"
            })
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª UX
        if ux["score"] < 75:
            recommendations.append({
                "category": "user_experience",
                "priority": "medium",
                "title": "Ø¨Ù‡Ø¨ÙˆØ¯ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ",
                "description": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†",
                "actions": [
                    "Ø·Ø±Ø§Ø­ÛŒ responsive",
                    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† navigation",
                    "Ø¨Ù‡Ø¨ÙˆØ¯ CTA buttons",
                    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ±Ù… ØªÙ…Ø§Ø³"
                ],
                "expected_improvement": "10-20% Ø¨Ù‡Ø¨ÙˆØ¯ engagement"
            })
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ
        if accessibility["score"] < 70:
            recommendations.append({
                "category": "accessibility",
                "priority": "medium",
                "title": "Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ",
                "description": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†",
                "actions": [
                    "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† alt text",
                    "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ARIA labels",
                    "Ø¨Ù‡Ø¨ÙˆØ¯ keyboard navigation",
                    "Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ØªØ±Ø§Ø³Øª Ø±Ù†Ú¯â€ŒÙ‡Ø§"
                ],
                "expected_improvement": "Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ 15% Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"
            })
        
        return recommendations
    
    def _calculate_overall_score(self, performance: Dict, seo: Dict, ux: Dict, accessibility: Dict) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ"""
        weights = {
            "performance": 0.3,
            "seo": 0.25,
            "user_experience": 0.25,
            "accessibility": 0.2
        }
        
        overall_score = (
            performance["score"] * weights["performance"] +
            seo["score"] * weights["seo"] +
            ux["score"] * weights["user_experience"] +
            accessibility["score"] * weights["accessibility"]
        )
        
        return round(overall_score, 1)
    
    def save_analytics_data(self, data: AnalyticsData) -> None:
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ"""
        self.analytics_data.append(data)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        data_file = "analytics_data.json"
        with open(data_file, "w", encoding="utf-8") as f:
            json.dump([
                {
                    "page_url": item.page_url,
                    "load_time": item.load_time,
                    "bounce_rate": item.bounce_rate,
                    "conversion_rate": item.conversion_rate,
                    "seo_score": item.seo_score,
                    "accessibility_score": item.accessibility_score,
                    "user_satisfaction": item.user_satisfaction,
                    "timestamp": item.timestamp.isoformat()
                }
                for item in self.analytics_data
            ], f, ensure_ascii=False, indent=2)
    
    def get_performance_trends(self, days: int = 30) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø±ÙˆÙ†Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_data = [d for d in self.analytics_data if d.timestamp >= cutoff_date]
        
        if not recent_data:
            return {"error": "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§
        avg_load_time = sum(d.load_time for d in recent_data) / len(recent_data)
        avg_bounce_rate = sum(d.bounce_rate for d in recent_data) / len(recent_data)
        avg_conversion_rate = sum(d.conversion_rate for d in recent_data) / len(recent_data)
        avg_seo_score = sum(d.seo_score for d in recent_data) / len(recent_data)
        
        return {
            "period_days": days,
            "data_points": len(recent_data),
            "average_load_time": round(avg_load_time, 2),
            "average_bounce_rate": round(avg_bounce_rate, 3),
            "average_conversion_rate": round(avg_conversion_rate, 3),
            "average_seo_score": round(avg_seo_score, 1),
            "trend": "improving" if avg_load_time < 2.0 else "needs_improvement"
        }

# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡
if __name__ == "__main__":
    engine = PredictiveAnalyticsEngine()
    
    # ØªØ³Øª ØªØ­Ù„ÛŒÙ„ ÙˆØ¨â€ŒØ³Ø§ÛŒØª
    async def test_website_analysis():
        website_data = {
            "html": "<html><head><title>Test Site</title></head><body><h1>Welcome</h1><p>This is a test site.</p></body></html>",
            "css": "body { font-family: Arial; }",
            "javascript": "console.log('Hello World');",
            "images": [
                {"size": 100000, "alt_text": "Test image"},
                {"size": 200000, "alt_text": ""}
            ]
        }
        
        analysis = await engine.analyze_website_performance(website_data)
        print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ ÙˆØ¨â€ŒØ³Ø§ÛŒØª:")
        print(f"Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {analysis['overall_score']}")
        print(f"Ø¹Ù…Ù„Ú©Ø±Ø¯: {analysis['performance']['score']}")
        print(f"SEO: {analysis['seo']['score']}")
        print(f"ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ: {analysis['user_experience']['score']}")
        print(f"Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ¾Ø°ÛŒØ±ÛŒ: {analysis['accessibility']['score']}")
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª
    asyncio.run(test_website_analysis())
