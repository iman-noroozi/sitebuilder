#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Voice Interface - Revolutionary voice-controlled website building
The most advanced voice interface for website creation
"""

import json
import asyncio
import speech_recognition as sr
import pyttsx3
import threading
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import logging
import hashlib
import uuid
import openai
import whisper
import torch
from transformers import pipeline, AutoTokenizer, AutoModel

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VoiceCommand(Enum):
    """Voice command types"""
    CREATE_ELEMENT = "create_element"
    MODIFY_STYLE = "modify_style"
    ADD_CONTENT = "add_content"
    NAVIGATE = "navigate"
    SAVE_PROJECT = "save_project"
    EXPORT_WEBSITE = "export_website"
    AI_SUGGEST = "ai_suggest"
    COLLABORATE = "collaborate"
    VOICE_SEARCH = "voice_search"
    EMERGENCY_HELP = "emergency_help"

class VoiceEmotion(Enum):
    """Voice emotion detection"""
    HAPPY = "happy"
    FRUSTRATED = "frustrated"
    EXCITED = "excited"
    CONFUSED = "confused"
    CONFIDENT = "confident"
    TIRED = "tired"

@dataclass
class VoiceContext:
    """Voice context information"""
    user_id: str
    session_id: str
    language: str
    emotion: VoiceEmotion
    confidence: float
    timestamp: datetime
    previous_commands: List[str]
    current_focus: str

class AIVoiceInterface:
    """Revolutionary AI-powered voice interface"""
    
    def __init__(self):
        self.recognizer = sr.Recognizer()
        self.tts_engine = pyttsx3.init()
        self.voice_contexts: Dict[str, VoiceContext] = {}
        self.command_handlers: Dict[VoiceCommand, Callable] = {}
        self.ai_models = {}
        self.voice_patterns: Dict[str, List[str]] = {}
        self.emotion_detector = None
        self.whisper_model = None
        
        # Initialize AI voice interface
        self._initialize_voice_engine()
        self._initialize_ai_models()
        self._initialize_command_handlers()
        self._initialize_voice_patterns()
        
        logger.info("AI Voice Interface initialized")
    
    def _initialize_voice_engine(self):
        """Initialize voice recognition and synthesis"""
        # Configure speech recognition
        self.recognizer.energy_threshold = 300
        self.recognizer.dynamic_energy_threshold = True
        self.recognizer.pause_threshold = 0.8
        
        # Configure text-to-speech
        voices = self.tts_engine.getProperty('voices')
        for voice in voices:
            if 'english' in voice.name.lower() or 'persian' in voice.name.lower():
                self.tts_engine.setProperty('voice', voice.id)
                break
        
        self.tts_engine.setProperty('rate', 150)
        self.tts_engine.setProperty('volume', 0.9)
    
    def _initialize_ai_models(self):
        """Initialize AI models for voice processing"""
        try:
            # Initialize Whisper for speech-to-text
            self.whisper_model = whisper.load_model("base")
            
            # Initialize emotion detection
            self.emotion_detector = pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base"
            )
            
            # Initialize intent classification
            self.intent_classifier = pipeline(
                "text-classification",
                model="microsoft/DialoGPT-medium"
            )
            
            # Initialize language detection
            self.language_detector = pipeline(
                "text-classification",
                model="papluca/xlm-roberta-base-language-detection"
            )
            
        except Exception as e:
            logger.warning(f"Some AI models could not be loaded: {e}")
    
    def _initialize_command_handlers(self):
        """Initialize voice command handlers"""
        self.command_handlers = {
            VoiceCommand.CREATE_ELEMENT: self._handle_create_element,
            VoiceCommand.MODIFY_STYLE: self._handle_modify_style,
            VoiceCommand.ADD_CONTENT: self._handle_add_content,
            VoiceCommand.NAVIGATE: self._handle_navigate,
            VoiceCommand.SAVE_PROJECT: self._handle_save_project,
            VoiceCommand.EXPORT_WEBSITE: self._handle_export_website,
            VoiceCommand.AI_SUGGEST: self._handle_ai_suggest,
            VoiceCommand.COLLABORATE: self._handle_collaborate,
            VoiceCommand.VOICE_SEARCH: self._handle_voice_search,
            VoiceCommand.EMERGENCY_HELP: self._handle_emergency_help
        }
    
    def _initialize_voice_patterns(self):
        """Initialize voice command patterns"""
        self.voice_patterns = {
            "create_element": [
                "Ø§ÛŒØ¬Ø§Ø¯ Ø¹Ù†ØµØ±", "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†", "Ø³Ø§Ø®Øª", "create element", "add element",
                "make a", "build a", "insert", "Ø§ÙØ²ÙˆØ¯Ù†", "Ø³Ø§Ø®ØªÙ†"
            ],
            "modify_style": [
                "ØªØºÛŒÛŒØ± Ø§Ø³ØªØ§ÛŒÙ„", "ØªØºÛŒÛŒØ± Ø±Ù†Ú¯", "ØªØºÛŒÛŒØ± ÙÙˆÙ†Øª", "change style", "modify",
                "update style", "ØªØºÛŒÛŒØ± Ø¸Ø§Ù‡Ø±", "ØªØºÛŒÛŒØ± Ø·Ø±Ø§Ø­ÛŒ", "style change"
            ],
            "add_content": [
                "Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§", "Ù†ÙˆØ´ØªÙ† Ù…ØªÙ†", "Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø­ØªÙˆØ§", "add content",
                "write text", "insert content", "Ù…Ø­ØªÙˆØ§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†", "Ù†ÙˆØ´ØªÙ†"
            ],
            "navigate": [
                "Ø¨Ø±Ùˆ Ø¨Ù‡", "Ø±ÙØªÙ† Ø¨Ù‡", "navigate to", "go to", "move to",
                "Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡", "Ø±ÙØªÙ†", "navigate", "Ø¨Ø±Ùˆ"
            ],
            "save_project": [
                "Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡", "save project", "Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†", "save", "save it",
                "Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù†", "save the project", "Ø°Ø®ÛŒØ±Ù‡"
            ],
            "export_website": [
                "Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ¨â€ŒØ³Ø§ÛŒØª", "export website", "ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù†", "export",
                "Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø±ÙØªÙ†", "export the website", "ØµØ§Ø¯Ø± Ú©Ù†"
            ],
            "ai_suggest": [
                "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ AI", "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù‡ÙˆØ´Ù…Ù†Ø¯", "ai suggest", "smart suggestion",
                "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ù†", "suggest", "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù‡ÙˆØ´Ù…Ù†Ø¯", "ai help"
            ],
            "collaborate": [
                "Ù‡Ù…Ú©Ø§Ø±ÛŒ", "collaborate", "Ú©Ø§Ø± ØªÛŒÙ…ÛŒ", "team work", "Ù‡Ù…Ú©Ø§Ø±ÛŒ Ú©Ù†",
                "collaboration", "Ú©Ø§Ø± Ù…Ø´ØªØ±Ú©", "team collaboration"
            ],
            "voice_search": [
                "Ø¬Ø³ØªØ¬ÙˆÛŒ ØµÙˆØªÛŒ", "voice search", "Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†", "search", "find",
                "Ø¬Ø³ØªØ¬Ùˆ", "search for", "Ù¾ÛŒØ¯Ø§ Ú©Ù†", "look for"
            ],
            "emergency_help": [
                "Ú©Ù…Ú©", "help", "Ø±Ø§Ù‡Ù†Ù…Ø§", "guide", "Ú©Ù…Ú© Ú©Ù†", "help me",
                "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ", "assistance", "Ú©Ù…Ú© ÙÙˆØ±ÛŒ", "urgent help"
            ]
        }
    
    # 1. Advanced Voice Recognition
    async def listen_for_commands(self, user_id: str, language: str = "auto") -> Dict:
        """Listen for voice commands with AI processing"""
        try:
            with sr.Microphone() as source:
                # Adjust for ambient noise
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
                
                # Listen for audio
                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)
                
                # Convert speech to text
                if language == "auto":
                    text = await self._transcribe_with_whisper(audio)
                else:
                    text = self.recognizer.recognize_google(audio, language=language)
                
                # Process with AI
                result = await self._process_voice_command(user_id, text)
                
                return result
                
        except sr.WaitTimeoutError:
            return {"error": "No voice input detected"}
        except sr.UnknownValueError:
            return {"error": "Could not understand speech"}
        except Exception as e:
            logger.error(f"Voice recognition error: {e}")
            return {"error": str(e)}
    
    async def _transcribe_with_whisper(self, audio) -> str:
        """Transcribe audio using Whisper AI"""
        try:
            # Save audio to temporary file
            import tempfile
            import wave
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                with wave.open(tmp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(16000)
                    wav_file.writeframes(audio.get_wav_data())
                
                # Transcribe with Whisper
                result = self.whisper_model.transcribe(tmp_file.name)
                return result["text"]
                
        except Exception as e:
            logger.error(f"Whisper transcription error: {e}")
            return ""
    
    # 2. AI Command Processing
    async def _process_voice_command(self, user_id: str, text: str) -> Dict:
        """Process voice command with AI"""
        # Detect language
        language = await self._detect_language(text)
        
        # Detect emotion
        emotion = await self._detect_emotion(text)
        
        # Classify intent
        intent = await self._classify_intent(text)
        
        # Extract entities
        entities = await self._extract_entities(text)
        
        # Get or create voice context
        context = self._get_voice_context(user_id, language, emotion)
        
        # Process command
        command_result = await self._execute_voice_command(intent, entities, context)
        
        # Update context
        self._update_voice_context(user_id, text, intent)
        
        return {
            "text": text,
            "language": language,
            "emotion": emotion.value,
            "intent": intent,
            "entities": entities,
            "result": command_result,
            "context": context
        }
    
    async def _detect_language(self, text: str) -> str:
        """Detect language of voice input"""
        try:
            if self.language_detector:
                result = self.language_detector(text)
                return result[0]["label"]
            else:
                # Fallback language detection
                if any(char in text for char in "Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ"):
                    return "fa"  # Persian
                else:
                    return "en"  # English
        except Exception:
            return "en"
    
    async def _detect_emotion(self, text: str) -> VoiceEmotion:
        """Detect emotion from voice input"""
        try:
            if self.emotion_detector:
                result = self.emotion_detector(text)
                emotion_map = {
                    "joy": VoiceEmotion.HAPPY,
                    "anger": VoiceEmotion.FRUSTRATED,
                    "surprise": VoiceEmotion.EXCITED,
                    "fear": VoiceEmotion.CONFUSED,
                    "sadness": VoiceEmotion.TIRED,
                    "neutral": VoiceEmotion.CONFIDENT
                }
                return emotion_map.get(result[0]["label"], VoiceEmotion.CONFIDENT)
            else:
                return VoiceEmotion.CONFIDENT
        except Exception:
            return VoiceEmotion.CONFIDENT
    
    async def _classify_intent(self, text: str) -> VoiceCommand:
        """Classify voice command intent"""
        text_lower = text.lower()
        
        # Check patterns
        for command, patterns in self.voice_patterns.items():
            for pattern in patterns:
                if pattern.lower() in text_lower:
                    return VoiceCommand(command.upper())
        
        # Default to AI suggest if unclear
        return VoiceCommand.AI_SUGGEST
    
    async def _extract_entities(self, text: str) -> Dict[str, Any]:
        """Extract entities from voice command"""
        entities = {
            "element_type": None,
            "style_property": None,
            "style_value": None,
            "content": None,
            "location": None,
            "target": None
        }
        
        # Simple entity extraction (can be enhanced with NER models)
        text_lower = text.lower()
        
        # Element types
        element_types = ["button", "text", "image", "div", "section", "header", "footer"]
        for element in element_types:
            if element in text_lower:
                entities["element_type"] = element
                break
        
        # Style properties
        style_properties = ["color", "size", "font", "background", "margin", "padding"]
        for prop in style_properties:
            if prop in text_lower:
                entities["style_property"] = prop
                break
        
        # Colors
        colors = ["red", "blue", "green", "yellow", "black", "white", "Ù‚Ø±Ù…Ø²", "Ø¢Ø¨ÛŒ", "Ø³Ø¨Ø²"]
        for color in colors:
            if color in text_lower:
                entities["style_value"] = color
                break
        
        return entities
    
    # 3. Voice Command Handlers
    async def _handle_create_element(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle create element command"""
        element_type = entities.get("element_type", "div")
        
        # Create element with AI suggestions
        element_data = {
            "type": element_type,
            "id": str(uuid.uuid4()),
            "content": entities.get("content", ""),
            "styles": {},
            "created_by": "voice_command",
            "timestamp": datetime.now().isoformat()
        }
        
        # Apply AI styling based on context
        if context.emotion == VoiceEmotion.EXCITED:
            element_data["styles"]["animation"] = "bounce"
        elif context.emotion == VoiceEmotion.CONFIDENT:
            element_data["styles"]["opacity"] = "1"
        
        return {
            "action": "create_element",
            "data": element_data,
            "message": f"Created {element_type} element with AI styling"
        }
    
    async def _handle_modify_style(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle modify style command"""
        property_name = entities.get("style_property", "color")
        property_value = entities.get("style_value", "blue")
        
        style_update = {
            "property": property_name,
            "value": property_value,
            "applied_by": "voice_command",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "modify_style",
            "data": style_update,
            "message": f"Updated {property_name} to {property_value}"
        }
    
    async def _handle_add_content(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle add content command"""
        content = entities.get("content", "New content added by voice")
        
        # Generate AI content if needed
        if not content or content == "New content added by voice":
            content = await self._generate_ai_content(context)
        
        content_data = {
            "text": content,
            "language": context.language,
            "emotion": context.emotion.value,
            "generated_by": "ai_voice",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "add_content",
            "data": content_data,
            "message": f"Added content: {content[:50]}..."
        }
    
    async def _handle_navigate(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle navigate command"""
        target = entities.get("target", "home")
        
        navigation_data = {
            "target": target,
            "method": "voice_command",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "navigate",
            "data": navigation_data,
            "message": f"Navigating to {target}"
        }
    
    async def _handle_save_project(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle save project command"""
        save_data = {
            "user_id": context.user_id,
            "session_id": context.session_id,
            "saved_by": "voice_command",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "save_project",
            "data": save_data,
            "message": "Project saved successfully"
        }
    
    async def _handle_export_website(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle export website command"""
        export_data = {
            "format": "html",
            "optimized": True,
            "exported_by": "voice_command",
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "export_website",
            "data": export_data,
            "message": "Website exported successfully"
        }
    
    async def _handle_ai_suggest(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle AI suggest command"""
        suggestions = await self._generate_ai_suggestions(context)
        
        return {
            "action": "ai_suggest",
            "data": suggestions,
            "message": "AI suggestions generated"
        }
    
    async def _handle_collaborate(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle collaborate command"""
        collaboration_data = {
            "action": "start_collaboration",
            "user_id": context.user_id,
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "collaborate",
            "data": collaboration_data,
            "message": "Collaboration session started"
        }
    
    async def _handle_voice_search(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle voice search command"""
        search_query = entities.get("content", "search query")
        
        search_data = {
            "query": search_query,
            "language": context.language,
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "action": "voice_search",
            "data": search_data,
            "message": f"Searching for: {search_query}"
        }
    
    async def _handle_emergency_help(self, entities: Dict, context: VoiceContext) -> Dict:
        """Handle emergency help command"""
        help_data = {
            "type": "emergency_help",
            "user_id": context.user_id,
            "context": context.current_focus,
            "timestamp": datetime.now().isoformat()
        }
        
        # Provide contextual help
        help_message = await self._generate_contextual_help(context)
        
        return {
            "action": "emergency_help",
            "data": help_data,
            "message": help_message
        }
    
    # 4. AI Content Generation
    async def _generate_ai_content(self, context: VoiceContext) -> str:
        """Generate AI content based on voice context"""
        try:
            # Create prompt based on context
            prompt = f"""
            Generate website content in {context.language} language.
            User emotion: {context.emotion.value}
            Previous commands: {', '.join(context.previous_commands[-3:])}
            Current focus: {context.current_focus}
            
            Generate appropriate content:
            """
            
            # Use OpenAI or local model
            if hasattr(self, 'openai_client'):
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=100
                )
                return response.choices[0].message.content
            else:
                # Fallback content generation
                return self._generate_fallback_content(context)
                
        except Exception as e:
            logger.error(f"AI content generation error: {e}")
            return self._generate_fallback_content(context)
    
    def _generate_fallback_content(self, context: VoiceContext) -> str:
        """Generate fallback content when AI is unavailable"""
        content_templates = {
            VoiceEmotion.HAPPY: "Welcome to our amazing website!",
            VoiceEmotion.EXCITED: "Check out this incredible feature!",
            VoiceEmotion.CONFIDENT: "Professional content for your website",
            VoiceEmotion.CONFUSED: "Let me help you with this content",
            VoiceEmotion.FRUSTRATED: "Here's a solution for your needs",
            VoiceEmotion.TIRED: "Simple and clear content"
        }
        
        return content_templates.get(context.emotion, "New content added")
    
    async def _generate_ai_suggestions(self, context: VoiceContext) -> List[Dict]:
        """Generate AI suggestions based on context"""
        suggestions = []
        
        # Context-based suggestions
        if context.emotion == VoiceEmotion.CONFUSED:
            suggestions.append({
                "type": "help",
                "title": "Need Help?",
                "description": "I can guide you through the process",
                "action": "show_tutorial"
            })
        
        if context.emotion == VoiceEmotion.EXCITED:
            suggestions.append({
                "type": "feature",
                "title": "Try Advanced Features",
                "description": "Explore AI-powered design tools",
                "action": "show_advanced_features"
            })
        
        # Always include basic suggestions
        suggestions.extend([
            {
                "type": "template",
                "title": "Use Template",
                "description": "Start with a professional template",
                "action": "show_templates"
            },
            {
                "type": "ai_design",
                "title": "AI Design Assistant",
                "description": "Let AI help you design",
                "action": "start_ai_design"
            }
        ])
        
        return suggestions
    
    async def _generate_contextual_help(self, context: VoiceContext) -> str:
        """Generate contextual help message"""
        help_messages = {
            "create_element": "Say 'create button' or 'add text' to create elements",
            "modify_style": "Say 'change color to blue' or 'make it bigger'",
            "add_content": "Say 'add content' followed by what you want to write",
            "navigate": "Say 'go to' followed by where you want to go",
            "save_project": "Say 'save project' to save your work",
            "export_website": "Say 'export website' to download your site"
        }
        
        return help_messages.get(context.current_focus, "I'm here to help! What would you like to do?")
    
    # 5. Voice Context Management
    def _get_voice_context(self, user_id: str, language: str, emotion: VoiceEmotion) -> VoiceContext:
        """Get or create voice context for user"""
        if user_id not in self.voice_contexts:
            self.voice_contexts[user_id] = VoiceContext(
                user_id=user_id,
                session_id=str(uuid.uuid4()),
                language=language,
                emotion=emotion,
                confidence=0.8,
                timestamp=datetime.now(),
                previous_commands=[],
                current_focus="general"
            )
        
        return self.voice_contexts[user_id]
    
    def _update_voice_context(self, user_id: str, command: str, intent: VoiceCommand):
        """Update voice context after command execution"""
        if user_id in self.voice_contexts:
            context = self.voice_contexts[user_id]
            context.previous_commands.append(command)
            context.current_focus = intent.value
            context.timestamp = datetime.now()
            
            # Keep only last 10 commands
            if len(context.previous_commands) > 10:
                context.previous_commands = context.previous_commands[-10:]
    
    # 6. Text-to-Speech with Emotion
    def speak_response(self, text: str, emotion: VoiceEmotion = VoiceEmotion.CONFIDENT):
        """Speak response with emotional tone"""
        try:
            # Adjust TTS based on emotion
            if emotion == VoiceEmotion.EXCITED:
                self.tts_engine.setProperty('rate', 180)
                self.tts_engine.setProperty('volume', 1.0)
            elif emotion == VoiceEmotion.TIRED:
                self.tts_engine.setProperty('rate', 120)
                self.tts_engine.setProperty('volume', 0.7)
            elif emotion == VoiceEmotion.CONFIDENT:
                self.tts_engine.setProperty('rate', 150)
                self.tts_engine.setProperty('volume', 0.9)
            
            # Speak the text
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
            
        except Exception as e:
            logger.error(f"TTS error: {e}")
    
    # 7. Continuous Voice Monitoring
    async def start_continuous_listening(self, user_id: str):
        """Start continuous voice monitoring"""
        while True:
            try:
                result = await self.listen_for_commands(user_id)
                
                if "error" not in result:
                    # Process successful command
                    await self._handle_voice_result(user_id, result)
                
                # Small delay to prevent excessive processing
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Continuous listening error: {e}")
                await asyncio.sleep(1)
    
    async def _handle_voice_result(self, user_id: str, result: Dict):
        """Handle voice command result"""
        # Execute the command
        command_result = result.get("result", {})
        
        # Speak response
        if command_result.get("message"):
            self.speak_response(
                command_result["message"],
                VoiceEmotion(result.get("emotion", "confident"))
            )
        
        # Log the interaction
        logger.info(f"Voice command executed for user {user_id}: {result['intent']}")

# Example usage and testing
if __name__ == "__main__":
    # Initialize AI voice interface
    voice_interface = AIVoiceInterface()
    
    print("ðŸŽ¤ AI Voice Interface Demo")
    print("=" * 50)
    
    # Test voice recognition
    print("\n1. Testing voice recognition...")
    print("Say a command like 'create button' or 'change color to blue'")
    
    # Start continuous listening
    async def demo_voice_interface():
        await voice_interface.start_continuous_listening("demo_user")
    
    # Run the demo
    asyncio.run(demo_voice_interface())
